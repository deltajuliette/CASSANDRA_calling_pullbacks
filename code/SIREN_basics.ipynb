{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing SIREN\n",
    "**A tool for predicting pullbacks in equity markets**\n",
    "\n",
    "## Problem statement\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import pickle\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# Custom function\n",
    "import SIREN_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring our notebook remains tidy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of custom functions\n",
    "We will import our custom .py module (SIREN_func) which stores the following functions:\n",
    "* `eda_clean`: Kicking-starting the EDA process\n",
    "* `derive_yield_curves`: Calculating 10y3m, 10y2y, 5y2y, 30y2y, 30y3m for US and Euro-area regions\n",
    "* `roll_diff`: Calculate rolling differences for different time horizons\n",
    "* `roll_pct_chg`: Calculate rolling percentage changes for different time horizons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "The bulk of our data has been sourced from Bloomberg via the API on excel. We will write a loop to pull every sheet and store in a dictionary for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our dataset(s)\n",
    "xlsx = pd.ExcelFile('../data/spx_fundamentals.xlsx')\n",
    "\n",
    "# Reading all sheets to a mapsheet_to_df_map = {}\n",
    "empty_list = {}\n",
    "for i in xlsx.sheet_names:\n",
    "    empty_list[i] = pd.read_excel('../data/spx_fundamentals.xlsx', sheet_name=i, skiprows=11, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['econ_sur', 'usd', 'epu', 'finc', 'pe', 'pb', 'eq_indices', 'como', 'credit', 'pct52w', 'vol', 'aaii', 'us_cftc', 'put_call', 'us_yields', 'eu_yields', 'eurdollar', 'fra_ois'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview dictionary of dataframes\n",
    "empty_list.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning, EDA, Feature Engineering\n",
    "We will once again rely on a combination of loops and custom functions to clean and tidy our dataset(s). We will also create some features for the following datasets:\n",
    "\n",
    "* No engineering required (values-only; non-stationary)\n",
    "    - XX\n",
    "* Rolling differences across different horizons (stationary)\n",
    "    - Citi economic surprise indices (`econ_sur`)\n",
    "    - US, Euro-area rates (`us_yields`, `eu_yields`)\n",
    "        + Yield curves were also calculated\n",
    "    - Credit spreads (`credit`)\n",
    "* Rolling **percentage** changes across different horizons (stationary)\n",
    "    - Equity indices (`eq_indices`)\n",
    "    - USD indices (`usd`)\n",
    "    - Commodities (`como`)\n",
    "    - 12-month forward P/E ratios (`pe`)\n",
    "    - 12-month forward P/B ratios (`pb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "**Calculating differences across different horizons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_d = {}\n",
    "for df in ['econ_sur', 'credit', 'us_yields', 'eu_yields']:\n",
    "    if df in ['credit']:\n",
    "        roll_d[f'{df}_chg'] = SIREN_func.roll_diff(SIREN_func.fix_credit(empty_list[df]))\n",
    "    \n",
    "    elif df in ['us_yields', 'eu_yields']:\n",
    "        roll_d[f'{df}_chg'] = SIREN_func.roll_diff(SIREN_func.derive_yield_curves(empty_list[df]))\n",
    "\n",
    "    else:\n",
    "        roll_d[f'{df}_chg'] = SIREN_func.roll_diff(empty_list[df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating percentage changes across different time horizons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_d2 = {}\n",
    "for df in ['usd', 'pe', 'pb', 'como', 'eq_indices']:\n",
    "    if df in ['eq_indices']:\n",
    "        roll_d2[f'{df}_4w_return'] = SIREN_func.lag_roll_pct_chg(empty_list[df], 4)\n",
    "\n",
    "    else: \n",
    "        roll_d2[f'{df}_chg'] = SIREN_func.roll_pct_chg(empty_list[df]) # Tidy up with custom module later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storing the remaining time-series in a dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = {}\n",
    "for df in ['epu', 'finc', 'pct52w', 'vol', 'aaii', 'us_cftc', 'put_call']:\n",
    "    if df in ['us_cftc']:\n",
    "        d3[df] = SIREN_func.fix_cftc(empty_list[df])\n",
    "    \n",
    "    else: d3[df] = SIREN_func.adjust_dates_only(empty_list[df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {**roll_d, **roll_d2, **d3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['econ_sur_chg', 'credit_chg', 'us_yields_chg', 'eu_yields_chg', 'usd_chg', 'pe_chg', 'pb_chg', 'como_chg', 'eq_indices_4w_return', 'epu', 'finc', 'pct52w', 'vol', 'aaii', 'us_cftc', 'put_call'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tidying earnings revision ratios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .xlsx highlighting S&P 500's earnings revisions\n",
    "eri = pd.read_excel(\"../data/spx_eri.xlsx\", sheet_name=\"Combined\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe custom function to calculate ERIs and 4, 13-week differences\n",
    "eri_chg = SIREN_func.eri_diff(eri, 4, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all our datasets\n",
    "full = eri_chg.copy()\n",
    "for df in final_dict.keys():\n",
    "    full = pd.merge(left=full, right=final_dict[df], how='left', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "Shape of dataframe: (862, 285)\n",
      "% of Null values in dataframe: 1.19%\n",
      "% duplicate rows: 0.0%\n",
      "\n",
      "Column names: Index(['eri', 'eri_1m_chg', 'eri_3m_chg', 'cesiusd_1w_chg', 'cesieur_1w_chg',\n",
      "       'cesigbp_1w_chg', 'cesijpy_1w_chg', 'cesicny_1w_chg', 'cesiglf_1w_chg',\n",
      "       'cesiusd_4w_chg',\n",
      "       ...\n",
      "       'skew', 'aaii_bull', 'aaii_bear', 'aaii_neut', 'cftc_nc_net',\n",
      "       'cftc_nc_long', 'cftc_nc_short', 'cftc_oi', 'cftc_nc_net_pct_oi',\n",
      "       'cboe_us'],\n",
      "      dtype='object', length=285)\n",
      "Columns Count: \n",
      "float64    285\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Previewing our final dataset\n",
    "SIREN_func.eda_clean(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop all missing values\n",
    "full.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this down as .csv\n",
    "full.to_pickle('../data/full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "686ebd21d830b5e85077fe54ac44c708c86b5f3b5f3e0918a9879f98d13a9c29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
