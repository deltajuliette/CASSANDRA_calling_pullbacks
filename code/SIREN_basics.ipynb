{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing SIREN\n",
    "**A tool for predicting pullbacks in equity markets**\n",
    "\n",
    "## Problem statement\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno # Visualising our missing values\n",
    "import pickle # For exporting our dataset\n",
    "\n",
    "from pandas_profiling import ProfileReport # Automating the EDA process\n",
    "from datetime import timedelta\n",
    "\n",
    "# Custom function(s)\n",
    "import SIREN_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring our notebook remains tidy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customising aesthetics\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('icefire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "The bulk of our data has been sourced from Bloomberg via the API on excel. We will write a loop to pull every sheet and store in a dictionary for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset(s)\n",
    "* `spx_fundamentals.xlsx`\n",
    "* `spx_eri.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our dataset(s)\n",
    "xlsx = pd.ExcelFile('../data/spx_fundamentals.xlsx')\n",
    "\n",
    "# Reading all sheets to a mapsheet_to_df_map = {}\n",
    "empty_list = {}\n",
    "for i in xlsx.sheet_names:\n",
    "    empty_list[i] = pd.read_excel('../data/spx_fundamentals.xlsx', sheet_name=i, skiprows=11, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['econ_sur', 'usd', 'epu', 'finc', 'pe', 'pb', 'eq_indices', 'como', 'credit', 'pct52w', 'vol', 'aaii', 'us_cftc', 'put_call', 'us_yields', 'eu_yields', 'eurdollar', 'fra_ois'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview dictionary of dataframes\n",
    "empty_list.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring our data\n",
    "With the help of a really handy package (i.e. pandas_profiling), we can carry out our extensive EDA with just a few lines of code. We could always write a loop for this process, but I figured it'll be better to examine each sub-dataset individually to check for multi-collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 56/56 [00:10<00:00,  5.20it/s, Completed]                    \n",
      "Generate report structure: 100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n",
      "                                                             \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509203e5e5b24487a8970af46f4f0b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploring Citi Economic surprise indices\n",
    "cesi_profile = ProfileReport(empty_list['econ_sur'].set_index('date'), title=\"Citi Economic Surprise Indices\", explorative=True)\n",
    "cesi_profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts**: US economic surprises are highly correlated with Euro area, UK, China and global indices. We could technically leave Japan in, but we posit it will likely not have a significant impact on final model. \n",
    "\n",
    "**Action plan**: Keep `cesiusd` and drop the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 26/26 [00:04<00:00,  5.98it/s, Completed]                    \n",
      "Generate report structure: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "                                                             \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2647c0b27c4840fcbe9f024fad6f4466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploring USD indices\n",
    "usd_profile = ProfileReport(empty_list['usd'].set_index('date'), title=\"USD Indices\", explorative=True)\n",
    "usd_profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts**: No brainer. We're seeing significant multi-correlation here. There also appear to be repeated values for `usd_dxy` and `usd_bbdxy`.\n",
    "\n",
    "**Action plan**: Keep the most diverse index (`usd_twi`) and drop the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 56/56 [00:10<00:00,  5.28it/s, Completed]                       \n",
      "Generate report structure: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n",
      "                                                             \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8df4c7bf9524c11850e6cbd9b3c8479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploring 12-month forward Price-earnings ratios\n",
    "pe_profile = ProfileReport(empty_list['pe'].set_index('date'), title=\"PE ratios\", explorative=True)\n",
    "pe_profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts**: XX\n",
    "\n",
    "**Action plan**: XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of custom functions\n",
    "We will import our custom .py module (**SIREN_func**) which stores the following functions:\n",
    "* `eda_clean`: Provides a quick snapshot of our project\n",
    "* `derive_yield_curves`: Calculating 30y10ys, 30y5ys, 30y2ys, 30y3ms, 10y5ys, 10y2ys, 10y3ms, 5y2ys, 5y3ms, 2y3ms for US and Euro-area regions\n",
    "* `fix_credit`: Standardising credit spreads as pp; Calculating spread between US high-yield and investment-grade bonds\n",
    "* `fix_cftc`: Deriving CFTC net non-commercial positions as a % of total open interest\n",
    "* `eri_diff`: Derive earnings revision indices and rolling changes across different horizons (4, 13-week)\n",
    "* `roll_diff`: Calculate rolling differences for different time horizons (1, 4, 13, 26-week)\n",
    "* `lag_roll_pct_chg`: Lagging rolling percentage changes for various equity indices (4-week)\n",
    "* `roll_pct_chg`: Calculate rolling percentage changes for different time horizons (1, 4, 13, 26-week)\n",
    "* `adjust_dates_only`: Standardise dates for merging dataframes later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will once again rely on a combination of loops and custom functions to clean and tidy our dataset(s). We will also create some features for the following datasets:\n",
    "\n",
    "* No engineering required (values-only; non-stationary)\n",
    "    - XX\n",
    "* Rolling differences across different horizons (stationary)\n",
    "    - Citi economic surprise indices (`econ_sur`)\n",
    "    - US, Euro-area rates (`us_yields`, `eu_yields`)\n",
    "        + Yield curves were also calculated\n",
    "    - Credit spreads (`credit`)\n",
    "* Rolling **percentage** changes across different horizons (stationary)\n",
    "    - Equity indices (`eq_indices`)\n",
    "    - USD indices (`usd`)\n",
    "    - Commodities (`como`)\n",
    "    - 12-month forward P/E ratios (`pe`)\n",
    "    - 12-month forward P/B ratios (`pb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "**Calculating differences across different horizons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a empty dictionary to house our sub-dataset(s)\n",
    "roll_d = {}\n",
    "\n",
    "# Datasets of interest\n",
    "for df in ['econ_sur', 'credit', 'us_yields', 'eu_yields']:\n",
    "    if df in ['credit']:\n",
    "        roll_d[f'{df}_chg'] = SIREN_func.roll_diff(SIREN_func.fix_credit(empty_list[df]))\n",
    "    \n",
    "    elif df in ['us_yields', 'eu_yields']:\n",
    "        roll_d[f'{df}_chg'] = SIREN_func.roll_diff(SIREN_func.derive_yield_curves(empty_list[df]))\n",
    "\n",
    "    else:\n",
    "        roll_d[f'{df}_chg'] = SIREN_func.roll_diff(empty_list[df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating percentage changes across different time horizons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating another empty dictionary to house sub-dataset(s)\n",
    "roll_d2 = {}\n",
    "\n",
    "# Datasets of interest\n",
    "for df in ['usd', 'pe', 'pb', 'como', 'eq_indices']:\n",
    "    if df in ['eq_indices']:\n",
    "        roll_d2[f'{df}_4w_return'] = SIREN_func.lag_roll_pct_chg(empty_list[df], 4)\n",
    "\n",
    "    else: \n",
    "        roll_d2[f'{df}_chg'] = SIREN_func.roll_pct_chg(empty_list[df]) # Tidy up with custom module later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storing the remaining time-series in a dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating another empty dictionary to house sub-datasets\n",
    "d3 = {}\n",
    "\n",
    "# Datasets of interest\n",
    "for df in ['epu', 'finc', 'pct52w', 'vol', 'aaii', 'us_cftc', 'put_call']:\n",
    "    if df in ['us_cftc']:\n",
    "        d3[df] = SIREN_func.fix_cftc(empty_list[df])\n",
    "    \n",
    "    else: d3[df] = SIREN_func.adjust_dates_only(empty_list[df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging out dictionaries\n",
    "final_dict = {**roll_d, **roll_d2, **d3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['econ_sur_chg', 'credit_chg', 'us_yields_chg', 'eu_yields_chg', 'usd_chg', 'pe_chg', 'pb_chg', 'como_chg', 'eq_indices_4w_return', 'epu', 'finc', 'pct52w', 'vol', 'aaii', 'us_cftc', 'put_call'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out our keys to our sub-dataset(s)\n",
    "final_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tidying earnings revision ratios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .xlsx highlighting S&P 500's earnings revisions\n",
    "eri = pd.read_excel(\"../data/spx_eri.xlsx\", sheet_name=\"Combined\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe custom function to calculate ERIs and 4, 13-week differences\n",
    "eri_chg = SIREN_func.eri_diff(eri, 4, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all our datasets\n",
    "full = eri_chg.copy()\n",
    "for df in final_dict.keys():\n",
    "    full = pd.merge(left=full, right=final_dict[df], how='left', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "Shape of dataframe: (862, 285)\n",
      "% of Null values in dataframe: 1.19%\n",
      "% duplicate rows: 0.0%\n",
      "\n",
      "Column names: Index(['eri', 'eri_1m_chg', 'eri_3m_chg', 'cesiusd_1w_chg', 'cesieur_1w_chg',\n",
      "       'cesigbp_1w_chg', 'cesijpy_1w_chg', 'cesicny_1w_chg', 'cesiglf_1w_chg',\n",
      "       'cesiusd_4w_chg',\n",
      "       ...\n",
      "       'skew', 'aaii_bull', 'aaii_bear', 'aaii_neut', 'cftc_nc_net',\n",
      "       'cftc_nc_long', 'cftc_nc_short', 'cftc_oi', 'cftc_nc_net_pct_oi',\n",
      "       'cboe_us'],\n",
      "      dtype='object', length=285)\n",
      "Columns Count: \n",
      "float64    285\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Previewing our final dataset\n",
    "SIREN_func.eda_clean(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop all missing values\n",
    "full.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this down as .csv\n",
    "full.to_pickle('../data/full.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "686ebd21d830b5e85077fe54ac44c708c86b5f3b5f3e0918a9879f98d13a9c29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
